{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "# one_hot allow labels to be in ont_hot_encoded format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image dims\n",
    "input_width = 28\n",
    "input_height = 28\n",
    "\n",
    "# image channels\n",
    "input_channels = 1\n",
    "# image total pixels\n",
    "input_pixels = 784\n",
    "\n",
    "# units in cl layers\n",
    "n_conv1 = 32\n",
    "n_conv2 = 64\n",
    "\n",
    "# filter size -> consider to be square\n",
    "conv1_k = 5\n",
    "conv2_k = 5\n",
    "# stride size \n",
    "stride1 = 1\n",
    "stride2 = 1\n",
    "# pooling layer size\n",
    "max_pool1_k = 2\n",
    "max_pool2_k = 2\n",
    "# dense layer size\n",
    "n_hidden = 1024\n",
    "# no of classes\n",
    "n_out = 10\n",
    "\n",
    "input_size_to_hidden = ((input_width //(max_pool1_k*max_pool2_k))* (input_height //(max_pool1_k*max_pool2_k))*n_conv2)\n",
    "# at the end image size will be  -> width and height divide by pool sizes and channel will be the unit of last cl layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'wcl1' : tf.Variable(tf.random_normal([conv1_k,conv1_k,input_channels,n_conv1])),\n",
    "    # weights will be like -> filter widht , height , channles , unit in layer\n",
    "    'wcl2' : tf.Variable(tf.random_normal([conv2_k,conv2_k,n_conv1,n_conv2])),\n",
    "    # here channel will be the unit of previous layer\n",
    "    'wh1' : tf.Variable(tf.random_normal([input_size_to_hidden,n_hidden])),\n",
    "    'wo' : tf.Variable(tf.random_normal([n_hidden,n_out]))\n",
    "}\n",
    "biases = {\n",
    "    \n",
    "    'bcl1' : tf.Variable(tf.random_normal([n_conv1])),\n",
    "    'bcl2' : tf.Variable(tf.random_normal([n_conv2])),\n",
    "    'bh1' : tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'bo' : tf.Variable(tf.random_normal([n_out]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will define conv function -> basically process inside cl layer\n",
    "\n",
    "# here we will have 3 things to do -> you can skip other 2 if you want\n",
    "# 1) apply convulation process\n",
    "# 2) add biases\n",
    "# 3) apply activation function\n",
    "\n",
    "def conv(x, weights,bias, strides = 1):\n",
    "    out = tf.nn.conv2d(x , weights ,  padding = \"SAME\" , strides = [1,strides,strides,1])\n",
    "    # what is strides = [1,strides,strides,1] \n",
    "    #    => here first argument is -> how many image need to access -> pass 1 -> access one at time -> so that all can be covered\n",
    "    #    => here last argument is -> how many channel need to access \n",
    "    #                 -> pass 1 -> give result as channel to be 1\n",
    "    #  => here 2nd/3rd argument is -> how many pixel need to shift \n",
    "    #        -> after applying convulation on group of pixel -> width wise and height wise\n",
    "    \n",
    "    out = tf.nn.bias_add(out,bias) # adding biases\n",
    "    out = tf.nn.relu(out) # applying activation function\n",
    "    return out\n",
    "\n",
    "# maxPooling function\n",
    "def maxpooling(x , k = 2):\n",
    "    # here k is window size\n",
    "    return tf.nn.max_pool(x , padding = \"SAME\" ,ksize = [1,k,k,1] ,strides = [1,k,k,1])\n",
    "# as we want to shift our pool winow exactly by 2 thats why k is passed here in stride and kshize denotes the window size of \n",
    "# pooling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lets define function to build our model\n",
    "def cnn(x, weights , biases):\n",
    "    # lets reshape it into 2D \n",
    "    x = tf.reshape(x, shape=[-1,input_width,input_height,1])\n",
    "    # as we channel to be 1 and -1 referes that -> algo will calculate itself\n",
    "    \n",
    "    #colvolutional layer 1\n",
    "    conv1 = conv(x,weights['wcl1'],biases['bcl1'],stride1)\n",
    "    #max pooling 1\n",
    "    conv1_pool = maxpooling(conv1,max_pool1_k)\n",
    "    \n",
    "    # similarly for cl2\n",
    "    \n",
    "    conv2 = conv(conv1_pool,weights['wcl2'],biases['bcl2'],stride2)\n",
    "    conv2_pool = maxpooling(conv2,max_pool2_k)\n",
    "    \n",
    "    # here we got our output -> need to apply to dense layer\n",
    "    \n",
    "    # again need to reshape it in 1D\n",
    "    hidden_input = tf.reshape(conv2_pool, shape=[-1,input_size_to_hidden])\n",
    "        \n",
    "    hidden_output_before_activation = tf.add(tf.matmul(hidden_input,weights['wh1']),biases['bh1'])\n",
    "    hidden_output = tf.nn.relu(hidden_output_before_activation)\n",
    "    \n",
    "    output = tf.add(tf.matmul(hidden_output,weights['wo']),biases['bo'])\n",
    "    # at output layer -> applying default identity activation function\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refer code from neural network code in case of any explanation required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder('float',[None,input_pixels]) \n",
    "y = tf.placeholder(tf.int32,[None,n_out]) \n",
    "pred = cnn( x , weights , biases )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost =tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,labels = y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "optimize = optimizer.minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will run cost optimizer on batch gradient descent\n",
    "batch_size = 100\n",
    "for i in range(25):\n",
    "    num_batches = int(mnist.train.num_examples/batch_size)\n",
    "    # mnist.train.num_examples it will give size of train \n",
    "    \n",
    "    total_cost=0\n",
    "    for j in range(num_batches):\n",
    "        batch_x , batch_y = mnist.train.next_batch(batch_size)\n",
    "        # it will give us data in batches -> just provide batch size\n",
    "        c , _ =sess.run([cost,optimize],feed_dict={x:batch_x , y:batch_y})\n",
    "        total_cost += c\n",
    "    print(c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.argmax(pred,1)\n",
    "true_labels = tf.argmax(y,1)\n",
    "correct_predictions = tf.equal(predictions,true_labels)\n",
    "predictions,correct_predictions = sess.run([predictions,correct_predictions],feed_dict={ x: mnist.test.images, y:mnist.test.labels})\n",
    "correct_predictions.sum()\n",
    "# getting around 85% accuracy -> far better than before -> it was 10% but good for random initialization\n",
    "# getting around 96% accuracy -> when we have batch gradient descent  \n",
    "# it allow us to reach faster \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROPOUT LAYER\n",
    "\n",
    "\"\"\"\n",
    "Basically in order to prevent overfitting of weights of neural network\n",
    "\n",
    "we will perform regularization and to do so we will introduce dropout layer in between hidden layer\n",
    "\n",
    "what it will do -> it will drop some percentage of parameters lets say 0.2% of weights are drop\n",
    "\n",
    "then it will not be considered in forward and backward propagation as well\n",
    "\n",
    "and we assume that our model is smart enough to perform this operation -> and it will increase the accuracy\n",
    "\n",
    "\n",
    "with the help of regularization -> accuracy may inc or dec depend on dropout layer droppping technique.\n",
    "\n",
    "\n",
    "as we use it only at training time therefore at testing we will pass -> keep_prob = 1\n",
    "i.e keep all weights and biases\n",
    "\n",
    "in training we will pass -> 0.8 i.e keep 0.8 % of parameters else ignore it.\n",
    "\n",
    "\n",
    "it has syntax like this.\n",
    "tf.nn.drop_out(hidden_output , keep_prob) \n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
